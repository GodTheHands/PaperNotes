# 适用于 Hadoop 2.x 的 MapReduce 性能模型

## 摘要

### 创新内容

提出了基于 Hadoop 1.x 的现有性能模型的解决方案，但是考虑了架构更改，并使用排队网络模型捕获 MapReduce 作业的执行流程。

### 论文成果

通过将我们的模型估计与实际 Hadoop 2.x 设置中的测量值进行比较，验证了我们解决方案的准确性。

## 结论

我们检查了 map 和 reduce 任务持续时间的分布类型，并得出结论：任务持续时间具有高斯分布。基于这个结论，我们提出了第三种估计工作响应时间的方法，它比基于 Tripathi 的方法给我们更好的结果。

我们的实验显示了我们方法的有效性：标准区块大小的作业响应时间估计的平均误差在 11% 到 13.5% 的范围内。

## 相关工作

### 静态 MapReduce 性能模型

为了估计 Hadoop 1.x 中 MapReduce 作业的执行情况，在对任务阶段进行建模方面付出了巨大的努力并取得了重要的成果。Herodotou 提出了性能成本模型，用于描述 Hadoop 1.x 中 MapReduce 作业的执行。在他的论文中，性能模型在 map 和 reduce 任务中更精细的阶段粒度上描述了数据流和成本信息。它捕获 Map 任务的以下阶段：read、map、collect、spill 和 merge。对于 Reduce 任务，shuffle 阶段、merge 阶段和 reduce 和 write 阶段有独立的公式。就 Herodotou 的模型而言，总体任务执行时间只是所有 map 和 reduce 阶段的成本之和。正如我们在这些成本公式中看到的那样，每个 Map 和 Reduce 任务都有固定数量的槽，因为在 Hadoop 的第一个版本中，Map 和 Reduce 作业的资源数量是预先确定的，不会改变。YARN 完全脱离了 maps 和 reduce 的资源静态分区，没有静态的 slot 配置。因此，我们不能直接应用 Herodotou 的成本公式，有必要寻找其他方法。

此外，也有一些工作定义作业完成时间的下限和上限，并为作业提供资源分配，以便它在要求的截止日期内完成。

也有人尝试评估任务调度对系统性能的影响。但是，当前的调度程序既不打包任务，也不考虑所有相关的资源需求。这会导致资源碎片化和过度分配，因此，它会显着降低整体性能。Robert Grandl 等人提出了 Tetris，这是一个多资源集群调度器，它根据所有资源类型的需求将任务打包到节点，从而避免了现有调度器的主要限制。打包的目标是最大限度地提高任务吞吐量并加快作业完成速度。因此，Tetris 结合了两种启发式方法——最佳打包和最短剩余作业时间——以减少平均作业完成时间。作者证明，实现所需的公平性可以与提高集群性能共存。该调度程序是在 YARN 中实现的，并且显示最大完成时间和作业完成时间增加了 30% 以上。基于新的调度器，作者提出了一个具有几个缺点的性能模型。首先，快速求解器只在少数具有非线性约束的特殊情况下为人所知，然而在这里有多个约束是非线性的：资源延展性、任务放置以及任务持续时间与多台机器分配的资源的关系。找到最佳分配的计算代价很高。调度理论表明，即使消除了放置考虑因素，多维装箱问题也是 APX 难的。其次，在 MapReduce 作业中，忽略任务之间的依赖关系是不可接受的，比如 shuffle/sort 阶段可以在第一个 map 任务完成时开始。

### 动态 MapReduce 性能模型

为 MapReduce 作业开发成本模型的主要挑战是，它们必须以合理的准确性捕获作业可能遇到的各种延迟来源。具体而言，属于作业的任务可能会遇到两种类型的延迟：由于共享资源争用而导致的排队延迟，以及由于在同一作业中协作的任务（map 和 reduce 阶段）之间的优先约束而导致的同步延迟。有两种主要技术可以估计并行应用程序的工作负载的性能，这些技术本身不考虑同步延迟。其中一种技术是平均值分析 （MVA），它仅考虑由于共享公共资源而导致的任务排队延迟。因此，它不能直接应用于具有优先约束的工作负载，例如属于同一 MapReduce 作业的 map 和 reduce 任务之间的同步。另一种经典解决方案是联合利用马尔可夫链来表示系统的可能状态，并对网络模型进行排队，以计算状态之间的转换速率。但是，此类方法不能很好地扩展，因为状态空间会随着任务数量的增加呈指数级增长，因此无法应用于具有许多任务的模型作业，而 MapReduce 作业就是这种情况。

## 实验

### 实验设置

集群中的每个节点具有相同的技术特征：

- 2 个 Intel Xeon E5-2630L v2 @ 2.40 GHz
- 128 GB 内存 RAM
- 1 个硬盘 1 TB SATA-3
- 4 网络 Intel 千兆以太网

我们进行了一组实验，分析了根据以下参数使用三种不同方法估计的工作负载响应时间：

- 节点数目：4、6、8；
- 输入数据大小：1GB、5GB；
- 集群中并发执行的作业数：1、2、3、4。

对于每个实验，我们分析了固定三个参数中的两个的任务响应时间。每个实验重复 5 次，并在图表中绘制响应时间的中位数。

### 实验结果

所有实验都针对 2 种类型的工作完成：（a）字数统计和（b）排序。我们描述了响应时间值，同时考虑了三种估计作业响应时间的方法。首先，我们根据集群中节点数量的增加和固定的工作负载来评估性能模型的准确性，即对于每个实验，我们固定并发作业的数量和输入数据的大小（参见图 13 和图 14）。

![Fig13](.\imgs\MapReduce Performance Model for Hadoop 2.x\Fig13.jpeg)

![Fig14](.\imgs\MapReduce Performance Model for Hadoop 2.x\Fig14.jpeg)

我们可以注意到，基于 Fork/join 的方法和基于任务响应时间正态分布的方法提供了更准确的作业响应时间估计，误差在 11% 到 14.5% 之间，而基于 Tripathi 的方法显示误差在 18% 到 22% 之间。对于 5GB 的输入大小，我们得到的误差值较大：分别为 14.5% 和 22%。我们观察到，算法的准确性取决于 map 任务的数量，而不是直接取决于输入数据的大小。因此，观察到的估计误差差异与优先树的复杂性（最大深度）有关，优先级树的复杂度随着映射任务数量的增加而增加。为了证明这个假设，我们增加了 map 任务的数量而不增加输入数据大小。因此，我们将 map 任务的默认块大小从 128MB 减少到 64MB 并重复实验。图 13（f）和图 14（f）分别显示了输入数据大小等于 5GB 和作业数等于 1 的字数统计和排序的这些结果。

正如这些结果所表明的，实验证实了我们的假设，因为我们获得了最大的误差值：Fork/join、Normal Dist 和基于 Tripathi 的方法分别为 15%、17% 和 23%。虽然精度不直接取决于输入大小，但应根据输入数据的大小和聚类特征合理选择块大小。为了减小优先树的最大深度，从而减少误差，我们平衡构建的优先树。

在我们的模型中，Fork/join 方法和基于任务持续时间正态分布的方法比 Hadoop 1 的原始模型提高了准确性，我们的解决方案基于该模型。对于集群中的一个作业，我们获得了有竞争力的错误率结果。

总之，我们可以注意到，基于 Fork/join 的方法和基于任务持续时间正态分布的方法比基于 Tripathi 的方法提供了更准确的结果，但对于这三种方法，我们高估了执行时间。